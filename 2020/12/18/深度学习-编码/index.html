<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="本文将用2种语言来完成上一篇深度神经网络的编码。包括正向的模型预测，以及训练算法。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习 Step By Step (二，实战篇)">
<meta property="og:url" content="http://example.com/2020/12/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BC%96%E7%A0%81/index.html">
<meta property="og:site_name" content="Xin&#39;s Blog">
<meta property="og:description" content="本文将用2种语言来完成上一篇深度神经网络的编码。包括正向的模型预测，以及训练算法。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/normals.png">
<meta property="og:image" content="http://example.com/images/normal.png">
<meta property="og:image" content="http://yann.lecun.com/exdb/mnist/">
<meta property="article:published_time" content="2020-12-18T02:02:00.000Z">
<meta property="article:modified_time" content="2020-12-22T09:14:18.600Z">
<meta property="article:author" content="Shaoxin Yin">
<meta property="article:tag" content="矩阵运算 深度学习 梯度下降 反向传播 正则化  特征缩放">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/normals.png">

<link rel="canonical" href="http://example.com/2020/12/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BC%96%E7%A0%81/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>深度学习 Step By Step (二，实战篇) | Xin's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Xin's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">I love youbai</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%BC%96%E7%A0%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/header.jpg">
      <meta itemprop="name" content="Shaoxin Yin">
      <meta itemprop="description" content="A programmer's mind">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Xin's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习 Step By Step (二，实战篇)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-12-18 10:02:00" itemprop="dateCreated datePublished" datetime="2020-12-18T10:02:00+08:00">2020-12-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-12-22 17:14:18" itemprop="dateModified" datetime="2020-12-22T17:14:18+08:00">2020-12-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-DNN-java-python/" itemprop="url" rel="index"><span itemprop="name">深度学习 DNN java  python</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本文将用2种语言来完成上一篇深度神经网络的编码。包括正向的模型预测，以及训练算法。</p>
<span id="more"></span>
<h1 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h1><p>如果你想通过for循环，也不是不可以。对于python而言，矩阵运算已经有很好的库（numpy）了。java我没找到，手写一下，加深对线性代数的理解吧。首先抛开实际问题，专注于矩阵运算。对矩阵的了解对后面极为重要。</p>
<h2 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h2><p>向量是在矩阵之前我们需要了解的，对于之前我们认识的数字，可以成为标量。向量除了有大小，还有方向。如果在二维空间，很好理解，2个数字可以组成一个向量，例如（2，3）。<br>这个向量方可以理解为沿着x轴走2个单位，沿着y轴走3个单位，那么从原点0，到最终点的这一个线段，就是（2，3）所表示的向量，除了距离外，还有一个重要的属性，那就是方向。<br>我们在证明梯度下降最大的方向时候使用的柯西斯瓦茨不等式就是考虑的向量。在实际运用中，特征并不是一个，而是多个特征相互影响，那么这些特征就是一个向量。</p>
<h2 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h2><p>在几何意义上，矩阵就是向量的空间变换法则。这里，我们只需记住，权重如果用矩阵来衡量，是最简单不过的。我们只需知道有几行几列。矩阵就是一组有位置的数字。在这里定义矩阵<br>的类，实际上也涵盖了向量。因为我们可以把向量看成是nx1 的矩阵。也就是n行一列（当然也可以认为是1xn，这里统一下，向量用列的多）。</p>
<ul>
<li><p>加法减法<br>两个矩阵必须维度相同。对应位置相加或相减，得到新的矩阵。矩阵内部是一个二维数组。有行与列属性。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Matrix <span class="title function_">add</span><span class="params">(Matrix m)</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (m == <span class="literal">null</span>) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(<span class="string">&quot;matrix can&#x27;t be null&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> ((m.row != <span class="built_in">this</span>.row) || (m.column != <span class="built_in">this</span>.column)) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(String.format(<span class="string">&quot;matrix size %sx%s is not fit for size %sx%s&quot;</span>, <span class="built_in">this</span>.row, <span class="built_in">this</span>.column</span><br><span class="line">                  , m.row, m.column));</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="type">Matrix</span> <span class="variable">newMatrix</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Matrix</span>(row, column);</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; row; i++) &#123;</span><br><span class="line">          <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; column; j++) &#123;</span><br><span class="line">              newMatrix.data[i][j] = <span class="built_in">this</span>.data[i][j] + m.data[i][j];</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> newMatrix;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>乘法<br>如果乘法中一个是标量，则乘以每一个矩阵中的数，得到一个新的矩阵。<br>如果两个都是矩阵AXB，那么要求A的列必须等于B的行数,最终得到的新矩阵是 A 的行数， B的列数<br>乘法的原则是，A的行上的数乘以对应的B的列上的数，求和后放在A行数所决定的行，B列数所决定的列上。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Matrix <span class="title function_">multiply</span><span class="params">(Matrix m)</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (m == <span class="literal">null</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(<span class="string">&quot;matrix can&#x27;t be null&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">this</span>.column != m.row) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(String.format(<span class="string">&quot;matrix size %sx%s is not fit for size %sx%s&quot;</span>, <span class="built_in">this</span>.row, <span class="built_in">this</span>.column</span><br><span class="line">                , m.row, m.column));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">Matrix</span> <span class="variable">newMatrix</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Matrix</span>(<span class="built_in">this</span>.row, m.column);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="built_in">this</span>.row; i++) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; m.column; j++) &#123;</span><br><span class="line">            newMatrix.data[i][j] = rowMulColumn(i, j, m);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> newMatrix;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>点乘<br>要求维度一样，对应位置上的数相乘得到新的矩阵。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Matrix <span class="title function_">multiplyEach</span><span class="params">(Matrix m)</span> &#123;</span><br><span class="line">      <span class="keyword">if</span>(m==<span class="literal">null</span>)&#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(<span class="string">&quot;matrix can&#x27;t be null&quot;</span>);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (<span class="built_in">this</span>.row != m.row || <span class="built_in">this</span>.column != m.column) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(String.format(<span class="string">&quot;matrix size %sx%s is not fit for size %sx%s&quot;</span>, <span class="built_in">this</span>.row, <span class="built_in">this</span>.column</span><br><span class="line">                  , m.row, m.column));</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="type">Matrix</span> <span class="variable">newMatrix</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Matrix</span>(<span class="built_in">this</span>.row, <span class="built_in">this</span>.column);</span><br><span class="line">      <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; row; i++) &#123;</span><br><span class="line">          <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; column; j++) &#123;</span><br><span class="line">              newMatrix.data[i][j] = <span class="built_in">this</span>.data[i][j] * m.data[i][j];</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">return</span> newMatrix;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>转置<br>转置就是把每矩阵中一个元素的位置行列对调。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Matrix <span class="title function_">transpose</span><span class="params">()</span> &#123;</span><br><span class="line">     <span class="type">Matrix</span> <span class="variable">matrix</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Matrix</span>(column, row);</span><br><span class="line">     <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="built_in">this</span>.row; i++) &#123;</span><br><span class="line">         <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; <span class="built_in">this</span>.column; j++) &#123;</span><br><span class="line">             matrix.data[j][i] = <span class="built_in">this</span>.data[i][j];</span><br><span class="line">         &#125;</span><br><span class="line">     &#125;</span><br><span class="line">     <span class="keyword">return</span> matrix;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>基本上就这些了。其实使用的并不多。逆矩阵这些都没有说到。当然如果有学到卷积神经网络，可能还有个卷积。但是目前用到的基本就这些。<br>对于普通的函数应用，基本对于向量，矩阵来说都是对每个元素都应用函数即可。常见的log，exp等。</p>
<h1 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h1><p>对于权重偏置的初始化， 是一个比较重要的实现环节。虽然对于梯度下降算法，初始“位置”我们不是绝对要求，但是如果一开始就在一个相对不错的位置，<br>是能够加快下降的。这里首相想到一个正态分布（高斯分布）。这是自然界最常见的分布，如果我们的权重按照这个来分布，那么也许是一个不错的选择。<br>使用标准正态分布，均值为0，方差为1<br><img src="/images/normals.png" alt="标准正态分布"><br>关于0对称，方差决定的是图形的胖瘦<br>可以看看其他<br><img src="/images/normal.png" alt="正态分布"><br>绿色是标准正态分布</p>
<p>上代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 正态分布 高斯分布</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> u 均值</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> v 方差</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> n 数量</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@return</span> 返回指定数量的高斯分布数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="type">double</span>[] randn(<span class="type">double</span> u, <span class="type">double</span> v, <span class="type">int</span> n)&#123;</span><br><span class="line">    <span class="type">double</span>[] result = <span class="keyword">new</span> <span class="title class_">double</span>[n];</span><br><span class="line">    java.util.<span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">java</span>.util.Random();</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        result[i] = v*random.nextGaussian() + u;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>简化下，直接使用标准正态分布</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 均值为0， 方差为1 的标准正态分布</span></span><br><span class="line"><span class="comment">    * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">   <span class="keyword">public</span> <span class="keyword">static</span> <span class="type">double</span>[] randn(<span class="type">int</span> n)&#123;</span><br><span class="line">       <span class="type">double</span>[] result = <span class="keyword">new</span> <span class="title class_">double</span>[n];</span><br><span class="line">       java.util.<span class="type">Random</span> <span class="variable">random</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">java</span>.util.Random();</span><br><span class="line">       <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">           result[i] = random.nextGaussian();</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">return</span> result;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>接下来我们初始化权重，这里需要搞明白的就是，权重是两层神经元之间的连接，那么3层的模型，就有2组权重，也就是总的权重数量是层数减一<br>假设层次是 [4，3，2], 对应的 权重组是 2组，分别是 [(3x4), (2x3)] ，也就是连接下一层作为行，上一层是列。对于python，太简单了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sizes = [<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>]</span><br><span class="line">t = <span class="built_in">zip</span>(sizes[<span class="number">1</span>:], sizes[:-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<p>zip函数把两组数配对，返回一个元组列表。上面的代码就是说两个sizes一个去尾，一个掐头后组合在一起<br>t=[(3,4),(2,3)]<br>在结合标准正态分布，python的方法可以很简单</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">weights = [np.random.randn(x,y) <span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="built_in">zip</span>(sizes[<span class="number">1</span>:],sizes[:-<span class="number">1</span>])]</span><br><span class="line">biases = [np.random.randn(y,<span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br></pre></td></tr></table></figure>
<p>对于列表的生成，python是很方便。对于偏置，需要注意的是偏置从第二层开始才有。<br>来看看复杂的java代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">initBias</span><span class="params">(<span class="type">int</span>[] sizes)</span> &#123;</span><br><span class="line">    <span class="comment">//first layer is the input layer, there is no bias</span></span><br><span class="line">    Matrix bias[] = <span class="keyword">new</span> <span class="title class_">Matrix</span>[sizes.length - <span class="number">1</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; bias.length; i++) &#123;</span><br><span class="line">        <span class="comment">//bias is n row and 1 column</span></span><br><span class="line">        <span class="type">double</span>[] row = Distrubution.randn(sizes[i + <span class="number">1</span>]);</span><br><span class="line">        <span class="type">double</span>[][] biasData = <span class="keyword">new</span> <span class="title class_">double</span>[row.length][<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; j &lt; row.length; j++)&#123;</span><br><span class="line">            biasData[j][<span class="number">0</span>] = row[j];</span><br><span class="line">        &#125;</span><br><span class="line">        bias[i] = <span class="keyword">new</span> <span class="title class_">Matrix</span>(biasData);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">this</span>.bias = bias;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">initWeights</span><span class="params">(<span class="type">int</span>[] sizes)</span> &#123;</span><br><span class="line">    <span class="comment">//first layer is the input layer, there is no weight</span></span><br><span class="line">    <span class="comment">//weights may be multi columns</span></span><br><span class="line">    Matrix[] weights = <span class="keyword">new</span> <span class="title class_">Matrix</span>[sizes.length - <span class="number">1</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; sizes.length - <span class="number">1</span>; i++) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> <span class="variable">row</span> <span class="operator">=</span> sizes[i+<span class="number">1</span>];</span><br><span class="line">        <span class="type">int</span> <span class="variable">col</span> <span class="operator">=</span> sizes[i];</span><br><span class="line">        <span class="type">double</span>[] datas = Distrubution.randn(row * col);</span><br><span class="line">        <span class="type">double</span>[][] mdata = <span class="keyword">new</span> <span class="title class_">double</span>[row][col];</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">r</span> <span class="operator">=</span> <span class="number">0</span>; r &lt; row; r++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">c</span> <span class="operator">=</span> <span class="number">0</span>; c &lt; col; c++) &#123;</span><br><span class="line">                mdata[r][c] = datas[(r * col) + c];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        weights[i] = <span class="keyword">new</span> <span class="title class_">Matrix</span>(mdata);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">this</span>.weights = weights;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>不论如何，这里的思想是一致的，python的写法，真的省空间啊。</p>
<h1 id="正向模型"><a href="#正向模型" class="headerlink" title="正向模型"></a>正向模型</h1><p>正向的模型就是预测模型。给一组输入，计算出最终的输出。这里python 真的是更加方便了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">feedforward</span>(<span class="params">self,a</span>):</span><br><span class="line">    <span class="keyword">for</span> w, b <span class="keyword">in</span> <span class="built_in">zip</span>(weights, biases):</span><br><span class="line">        a = sigmoid(np.dot(w,a)+b)</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1.0</span> + np.exp(-z));</span><br></pre></td></tr></table></figure>
<p>太简洁了。激活函数作用下，使用了numpy 的dot方法，dot方法就是矩阵乘法。注意a，是列向量。<br>这时候java中的矩阵操作可以派上用场了</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> Matrix <span class="title function_">sigmoid</span><span class="params">(Matrix z)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> Matrix.exponentOfE(z.multiply(-<span class="number">1</span>)).add(<span class="number">1.0d</span>).divide(<span class="number">1.0d</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> Matrix <span class="title function_">feedforward</span><span class="params">(Matrix input)</span> &#123;</span><br><span class="line">        <span class="comment">//m is the size of input samples</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">m</span> <span class="operator">=</span> input.getColumn();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; sizes.length - <span class="number">1</span>; i++) &#123;</span><br><span class="line">            input = sigmoid(weight[i].multiply.add(bias[i].extendColumn(m)));</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> input;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">public</span> <span class="keyword">static</span> Matrix <span class="title function_">linear</span><span class="params">(Matrix input, Matrix weight)</span> &#123;</span><br><span class="line">          <span class="keyword">return</span> weight.multiply(input);</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>java的写法也还凑合。不过这里有些不一样，因为java的写法，是包含了样本数（也就是原来一列的一个样本特征，现在是n列了，可以自己计算下，发现通过矩阵运算，居然最后消失了。但是注意的是偏置不能，所以扩展了一个列，以便计算更加便捷，但是要注意在梯度计算的时候取平均值）。这里通过矩阵操作，我们发现，样本的数量并不会影响计算，但是对于偏置，需要扩展列（每一列都是一个样本）。这里需要注意的，也可以通过单条样本学习，然后通过for循环遍历每一批数据。</p>
<h1 id="随机批量梯度下降算法实现"><a href="#随机批量梯度下降算法实现" class="headerlink" title="随机批量梯度下降算法实现"></a>随机批量梯度下降算法实现</h1><p>第一步，随机，第二步，分批，第三部使用一批数据来实现梯度下降算法。</p>
<p>随机与分批可以同时完成，我们打乱样本，然后根据每一批的大小分批。python数据列表生成与分片非常方便</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  随机批量梯度下降算法 stochastic grandient descent</span></span><br><span class="line"><span class="comment">#  这里还有 mini-batch</span></span><br><span class="line"><span class="comment">#  完全的微分应用，类似求近似值。使用批量降低难度，通过迭代次数求得精度。</span></span><br><span class="line"><span class="comment">#  注意eta参数，不可过大，也不可太小。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">SGD</span>(<span class="params">self, training_data, epochs, mini_batch_size, eta, test_data=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Train the neural network using mini-batch stochastic graident descent. </span></span><br><span class="line"><span class="string">        The &quot;training_data&quot; is a list of tuples &quot;(x, y)&quot; representing the training </span></span><br><span class="line"><span class="string">        inputs and the desired outputs. The other non-optional parameters are self-explanatory.</span></span><br><span class="line"><span class="string">        If &quot;test_data&quot; is provided then the network will be evaluated against the test data </span></span><br><span class="line"><span class="string">        after eache epoch, and partial progress printed out. This is usefull for tracking progress,     but    slows thins down substantially.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> test_data:</span><br><span class="line">            n_test = <span class="built_in">len</span>(test_data)</span><br><span class="line">        n = <span class="built_in">len</span>(training_data)</span><br><span class="line">        <span class="comment"># 开始迭代，每一个epoch 完成所有training_data数据学习，并更新参数，通过测试参数来判断学习情况</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">            <span class="comment"># 打乱 随机</span></span><br><span class="line">            random.shuffle(training_data)</span><br><span class="line">            <span class="comment"># 根据批量大小从训练数据取出，每一批数据都包含mini_batch_size个训练数据</span></span><br><span class="line">            <span class="comment"># 从一个大数组里分割出n个小数组 ，没个数组里包含mini_batch_size个数据（最后一组不一定）</span></span><br><span class="line">            mini_batches = [training_data[k:k+mini_batch_size] <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, n, mini_batch_size)]</span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                self.update_mini_batch(mini_batch, eta)</span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Epoch %d: %d/%d&quot;</span>%(j, self.evaluate(test_data), n_test))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Epach %d complete&quot;</span>% j)</span><br></pre></td></tr></table></figure>
<p>分批在python里真的很容易。一行代码搞定。</p>
<p>最后是更新参数。这里用到了反向传播，下一节将会详细说明。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">update_mini_batch</span>(<span class="params">self, mini_batch, eta</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; Update the network&#x27;s weights and biases by applying gradient descent using backpropagation to a</span></span><br><span class="line"><span class="string">        single mini batch. The &#x27;&#x27;mini_batch&#x27;&#x27; is a list of tuples &#x27;&#x27;(x,y)&#x27;&#x27;, and &#x27;&#x27;eta&#x27;&#x27; is the learning rate.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="comment">#遍历每一个训练数据（数据对）</span></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            <span class="comment"># 通过反向传播算法，得到参数的梯度向量</span></span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.backprop(x,y)</span><br><span class="line"></span><br><span class="line">            nabla_b = [nb+dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> <span class="built_in">zip</span>(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw+dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> <span class="built_in">zip</span>(nabla_w, delta_nabla_w)]</span><br><span class="line">        <span class="comment">#累加每一条数据得到的参数，接下来使用梯度下降算法</span></span><br><span class="line">        <span class="comment">#随机批量梯度下降算法主要是在数据非常多的情况下，我们可以减少计算，加快下降。</span></span><br><span class="line">        <span class="comment"># 考虑到批量的均值，与整体的样本均值 差距不大。这也是我们要做shuffle的原因</span></span><br><span class="line">        <span class="comment"># w` = w - eta/n*nw </span></span><br><span class="line">        self.weights = [w-(eta/<span class="built_in">len</span>(mini_batch))*nw <span class="keyword">for</span> w, nw <span class="keyword">in</span> <span class="built_in">zip</span>(self.weights, nabla_w)]</span><br><span class="line">        self.biases = [b-(eta/<span class="built_in">len</span>(mini_batch))*nb <span class="keyword">for</span> b, nb <span class="keyword">in</span> <span class="built_in">zip</span>(self.biases, nabla_b)]</span><br></pre></td></tr></table></figure>
<p>这里可以看到 nabla_w 和 nabla_b 都在for循环中不断增加，在最后又要除以每批的大小。这就是求平均值。</p>
<p>我们用java实现时候，权重部分把批量大小带入矩阵，最后自然约去，所以不用求平均值。</p>
<p>来看看java的实现</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">SGD</span><span class="params">(TrainingData trainData, <span class="type">int</span> epochs, <span class="type">int</span> mini_batch_size, <span class="type">double</span> eta, <span class="type">double</span> lambda, TrainingData testData)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">testLen</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">trainLen</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span> (testData != <span class="literal">null</span>) &#123;</span><br><span class="line">            testLen = testData.getSize();</span><br><span class="line">        &#125;</span><br><span class="line">        trainLen = trainData.getSize();</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; epochs; i++) &#123;</span><br><span class="line">            <span class="comment">//shuffle training data</span></span><br><span class="line">            trainData.shuffle();</span><br><span class="line">            <span class="comment">//try to get mini batches</span></span><br><span class="line">            <span class="comment">//how to slice the array of Matrix ?</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">k</span> <span class="operator">=</span> <span class="number">0</span>; k &lt; trainLen; k += mini_batch_size) &#123;</span><br><span class="line">                <span class="type">int</span> <span class="variable">to</span> <span class="operator">=</span> k + mini_batch_size;</span><br><span class="line">                <span class="keyword">if</span> (k + mini_batch_size &gt; trainLen) &#123;</span><br><span class="line">                    to = trainLen;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="type">double</span>[][] inputData = Arrays.copyOfRange(trainData.getInput().getData(), k, to);</span><br><span class="line">                <span class="type">double</span>[][] resultData = Arrays.copyOfRange(trainData.getOutput().getData(), k, to);</span><br><span class="line"></span><br><span class="line">                <span class="comment">//make a new mini batch training data</span></span><br><span class="line">                updateMiniBatch(<span class="keyword">new</span> <span class="title class_">TrainingData</span>(inputData, resultData), eta, lambda, trainLen);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (testData != <span class="literal">null</span>) &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;Epoch &quot;</span> + i + <span class="string">&quot;: &quot;</span> + evaluate(testData) + <span class="string">&quot;\\&quot;</span> + testLen);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;Epoch complete &quot;</span> + i);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>java的shuffle方法是自定义的。有兴趣的同学可以自己实现</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">swap</span><span class="params">(<span class="type">double</span>[][] origin, <span class="type">int</span> i, <span class="type">int</span> j)</span> &#123;</span><br><span class="line">        <span class="type">double</span>[] row = <span class="keyword">new</span> <span class="title class_">double</span>[origin[i].length];</span><br><span class="line">        System.arraycopy(origin[i], <span class="number">0</span>, row, <span class="number">0</span>, row.length);</span><br><span class="line">        System.arraycopy(origin[j], <span class="number">0</span>, origin[i], <span class="number">0</span>, origin[i].length);</span><br><span class="line">        System.arraycopy(row, <span class="number">0</span>, origin[j], <span class="number">0</span>, origin[j].length);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">shuffle</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">double</span>[][] input = <span class="built_in">this</span>.getInput().getData();</span><br><span class="line">        <span class="type">double</span>[][] output = <span class="built_in">this</span>.getOutput().getData();</span><br><span class="line">        <span class="comment">//row, the row of input should be the same as output, they represent the features.</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">len</span> <span class="operator">=</span> getSize();</span><br><span class="line">        <span class="comment">// the size of input should be the same as the size of column in output.</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> len; i &gt; <span class="number">0</span>; i--) &#123;</span><br><span class="line">            <span class="comment">//random a item ,swap to the tail which is indicated by i.</span></span><br><span class="line">            <span class="type">int</span> <span class="variable">randomInt</span> <span class="operator">=</span> random.nextInt(i);</span><br><span class="line">            swap(input, randomInt, i - <span class="number">1</span>);</span><br><span class="line">            <span class="comment">//change the output according to input shuffle</span></span><br><span class="line">            swap(output, randomInt, i - <span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (!<span class="built_in">this</span>.isReady()) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>(<span class="string">&quot;shuffle &quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>更新权重的方法和python有略微区别。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">updateMiniBatch</span><span class="params">(TrainingData trainingMiniData, <span class="type">double</span> eta, <span class="type">double</span> lambda, <span class="type">double</span> n)</span> &#123;</span><br><span class="line">        <span class="type">Matrix</span> <span class="variable">input</span> <span class="operator">=</span> trainingMiniData.getInput();</span><br><span class="line">        <span class="type">Matrix</span> <span class="variable">output</span> <span class="operator">=</span> trainingMiniData.getOutput();</span><br><span class="line">        <span class="type">Parameter</span> <span class="variable">params</span> <span class="operator">=</span> backprop(input, output, lambda, n, trainingMiniData.getSize());</span><br><span class="line">        <span class="type">double</span> <span class="variable">len</span> <span class="operator">=</span> (<span class="type">double</span>)trainingMiniData.getSize();</span><br><span class="line">        <span class="comment">//after back propagation, update the parameters.</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="built_in">this</span>.sizes.length - <span class="number">1</span>; i++) &#123;</span><br><span class="line">            <span class="built_in">this</span>.bias[i] = <span class="built_in">this</span>.bias[i].subtract(params.getBias()[i].colSum().multiply(eta / len));</span><br><span class="line">            <span class="built_in">this</span>.weights[i] = <span class="built_in">this</span>.weights[i].subtract(params.getWeights()[i].multiply(eta/len));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>偏置这里我们有个colSum，这就是把每一列数据相加，合成一列。这个实现也很简单。</p>
<p>这种区别是因为我们把权重和偏置分开了，并使用了矩阵的优势。也可以合起来，合起来的问题是前向传播需要稍微麻烦些。理解起来也不够清晰。有兴趣的同学可以自己尝试。</p>
<p>无论是python还是java我们都有一个方法等待实现， backprop ，也就是反向传播算法。</p>
<h1 id="反向传播算法实现"><a href="#反向传播算法实现" class="headerlink" title="反向传播算法实现"></a>反向传播算法实现</h1><p>反向传播，在理论篇大家已经熟悉了。首相正向计算最后一层，然后计算最后一层误差，然后在计算前一层误差，</p>
<p>只到第二层，在来计算权重和偏置的梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backprop</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Return a tuple &#x27;&#x27;(nabla_b, nabla_w)&#x27;&#x27; representing the</span></span><br><span class="line"><span class="string">        gradient for the cost function C_x, &#x27;&#x27;nabla_b&#x27;&#x27; and &#x27;&#x27;nabla_w&#x27;&#x27;</span></span><br><span class="line"><span class="string">        are layer-by-layer lists of numpy arrays, similar to </span></span><br><span class="line"><span class="string">        &#x27;&#x27;self.biases&#x27;&#x27; and &#x27;&#x27;self.weights&#x27;&#x27;. &quot;&quot;&quot;</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line">        <span class="comment">#feedforward</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations=[x] <span class="comment">#list to store all the activations, layer by layer</span></span><br><span class="line">        zs = [] <span class="comment">#list to store all the z vectors layer by layer</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> <span class="built_in">zip</span>(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation)+b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = Network.sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line">        <span class="comment">#backward pass</span></span><br><span class="line">        <span class="comment">#输出层与其输入的误差计算</span></span><br><span class="line">        delta = (self.cost).delta(zs[-<span class="number">1</span>],activations[-<span class="number">1</span>],y)</span><br><span class="line">        nabla_b[-<span class="number">1</span>] =delta</span><br><span class="line">        nabla_w[-<span class="number">1</span>] = np.dot(delta, activations[-<span class="number">2</span>].transpose())</span><br><span class="line">        <span class="comment"># 开始倒数第二层一直到正数第二层，求误差</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            sp = Network.sigmoid_prime(z)</span><br><span class="line">            delta = np.dot(self.weights[-l+<span class="number">1</span>].transpose(), delta)*sp</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l-<span class="number">1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> (nabla_b, nabla_w)</span><br></pre></td></tr></table></figure>
<p>具体编码的时候，我们就设计到代价函数的选择，激活函数，以及激活函数的导数等。如果理论没有忘记，这里很好理解。这里把每一层的结果都存在了activations数组中。delta函数就是激活函数求导。这里for语句反向计算误差的方法利用了python的反向索引，负号来实现，很是精妙。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 激活函数 np.exp是自然对数， python天生支持向量操作啊</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">        <span class="keyword">return</span> (<span class="number">1.0</span>/(<span class="number">1.0</span> + np.exp(-z)))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid_prime</span>(<span class="params">z</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 这是对激活函数求导&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> Network.sigmoid(z)*(<span class="number">1</span>-Network.sigmoid(z))</span><br></pre></td></tr></table></figure>
<p>虽然二次代价函数不经常使用，我们这里还是给出两种代价函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">QuadraticCost</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fn</span>(<span class="params">a, y</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.5</span>*np.linalg.norm(a-y)**<span class="number">2</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">delta</span>(<span class="params">z, a, y</span>):</span><br><span class="line">        <span class="keyword">return</span> (a-y) * Network.sigmoid_prime(z)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CrossEntropyCost</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fn</span>(<span class="params">a, y</span>):</span><br><span class="line">        <span class="keyword">return</span> np.<span class="built_in">sum</span>(np.nan_to_num(-y*np.log(a)-(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-a))) </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">delta</span>(<span class="params">z, a, y</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 为了函数接口一致，z虽然没用到，也作为参数 &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> (a-y)</span><br></pre></td></tr></table></figure>
<p>这里numpy库为我们提供了线性代数模块linalg， normal是求范数，默认二范数，就是整体求平方和开根号。</p>
<p>nan_to_num是避免出现nan类型，用0替代。</p>
<p>来看看java的代码实现，</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> Parameter <span class="title function_">backprop</span><span class="params">(Matrix input, Matrix output, <span class="type">double</span> lambda, <span class="type">double</span> n, <span class="type">int</span> m)</span> &#123;</span><br><span class="line">        <span class="comment">//a row is a sample input data, need make a column is a sample input data, so column is the size of samples</span></span><br><span class="line">        <span class="type">Matrix</span> <span class="variable">activation</span> <span class="operator">=</span> input.transpose();</span><br><span class="line">        output = output.transpose();</span><br><span class="line">        <span class="type">int</span> <span class="variable">layer</span> <span class="operator">=</span> <span class="built_in">this</span>.sizes.length;</span><br><span class="line">        Matrix detBias[] = <span class="keyword">new</span> <span class="title class_">Matrix</span>[<span class="built_in">this</span>.bias.length];</span><br><span class="line">        Matrix detWeights[] = <span class="keyword">new</span> <span class="title class_">Matrix</span>[<span class="built_in">this</span>.weights.length];</span><br><span class="line">        Matrix[] activations = <span class="keyword">new</span> <span class="title class_">Matrix</span>[layer];</span><br><span class="line">        activations[<span class="number">0</span>] = Matrix.copy(activation);</span><br><span class="line">        Matrix[] layerWithInput = <span class="keyword">new</span> <span class="title class_">Matrix</span>[layer - <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; layer - <span class="number">1</span>; i++) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">batchSize</span> <span class="operator">=</span> activation.getColumn();</span><br><span class="line">            <span class="type">Matrix</span> <span class="variable">z</span> <span class="operator">=</span> linear(activation, <span class="built_in">this</span>.weights[i]).add(bias[i].extendColumn(batchSize));</span><br><span class="line">            layerWithInput[i] = z;</span><br><span class="line">            activation = sigmoid(z);</span><br><span class="line">            activations[i+<span class="number">1</span>] = activation;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="type">Matrix</span> <span class="variable">lastLayerOutput</span> <span class="operator">=</span> activations[layer-<span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * according  to the back propagation, the last error is computed by</span></span><br><span class="line"><span class="comment">         * the following method</span></span><br><span class="line"><span class="comment">         * e[L] = (dC/da).d(sigmoid)</span></span><br><span class="line"><span class="comment">         * if we use the quadratic function as cost function, the derivative is</span></span><br><span class="line"><span class="comment">         * a[L] - Y</span></span><br><span class="line"><span class="comment">         * here the Y is the real result</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        <span class="type">Matrix</span> <span class="variable">errors</span> <span class="operator">=</span> <span class="built_in">this</span>.cost.derivative(layerWithInput[layer-<span class="number">2</span>], lastLayerOutput, output);</span><br><span class="line">        detBias[detBias.length-<span class="number">1</span>] = Matrix.copy(errors);</span><br><span class="line">        detWeights[detWeights.length-<span class="number">1</span>] =  errors.multiply(activations[layer-<span class="number">2</span>].transpose());</span><br><span class="line"></span><br><span class="line">        <span class="comment">/** the error of last layer  has been computed.</span></span><br><span class="line"><span class="comment">         *  so layer - 3 is the second error in reversed-order</span></span><br><span class="line"><span class="comment">         **/</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> layer - <span class="number">3</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line"><span class="comment">//</span></span><br><span class="line">            errors = weights[i+<span class="number">1</span>].transpose().multiply(errors).multiplyEach(costPrime(layerWithInput[i]));</span><br><span class="line">            detBias[i] = Matrix.copy(errors);</span><br><span class="line">            detWeights[i] = errors.multiply(activations[i].transpose());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//set bias and weights</span></span><br><span class="line">        <span class="type">Parameter</span> <span class="variable">parameter</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Parameter</span>();</span><br><span class="line">        parameter.setBias(detBias);</span><br><span class="line">        parameter.setWeights(detWeights);</span><br><span class="line">        <span class="keyword">return</span> parameter;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>这里需要注意的是，程序语言的数组开始是0， 理论中是从1开始。所以这里要注意。</p>
<p>对于n层的神经网络，将会有n-1个输入，n-1个权重矩阵，n-1个误差。这些细节是要注意的。</p>
<p>交叉熵的java实现</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CrossEntropyCost</span> <span class="keyword">implements</span> <span class="title class_">CostFunction</span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">double</span> <span class="title function_">func</span><span class="params">(Matrix a, Matrix y)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> a.multiply(-<span class="number">1</span>).multiplyEach(a.log()).subtract(y.subtractBy(<span class="number">1</span>).multiplyEach(a.subtractBy(<span class="number">1</span>).log())).sum();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> Matrix <span class="title function_">derivative</span><span class="params">(Matrix z, Matrix a, Matrix y)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> a.subtract(y);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>至此，一个基本的深度神经网络系统就可以使用了。然后找个项目来试验下。</p>
<h1 id="手写数字识别"><a href="#手写数字识别" class="headerlink" title="手写数字识别"></a>手写数字识别</h1><p>MNIST这个手写数字库，真的是非常的好用，不大不小。感谢Yann.Lecun</p>
<p>我们可以从这里下载  <img src="http://yann.lecun.com/exdb/mnist/" alt="MNIST"></p>
<p>MNIST 数据集是基于 NIST（美国国家标准与技术研究院）收集的两个数据集合。为了构建 MNIST，</p>
<p>NIST 数据集合被 Yann LeCun，Corinna Cortes 和 Christopher J. C. Burges 拆分放⼊⼀个更⽅便的格式。</p>
<p>MNIST的数据是描述的28x28像素的图片，每一个图片都是一个手写数字。这些数字来源不同的人。我们分成了两部分</p>
<p>一部分用来训练，一部分用来测试。大概一共有60000个训练数据集，包括他的label（也就是正确的分类）。</p>
<p>如果细心的同学会发现，我们的java代码z红的trainingData  在使用的时候，做了转置操作。这是因为我们把数据读取到了一行中。通过转置，行变成了列，而我们的理论都是基于列来推理的。</p>
<p>先看看如何读取这些数据。 对于训练数据train-images-idx3-ubyte</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">[offset]</th>
<th style="text-align:left">[type]</th>
<th style="text-align:left">[value]</th>
<th style="text-align:left">[description]</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">0000</td>
<td style="text-align:left">32 bit integer</td>
<td style="text-align:left">0x00000803(2051)</td>
<td style="text-align:left">magic number  </td>
</tr>
<tr>
<td style="text-align:left">0004</td>
<td style="text-align:left">32 bit integer</td>
<td style="text-align:left">60000</td>
<td style="text-align:left">number of images  </td>
</tr>
<tr>
<td style="text-align:left">0008</td>
<td style="text-align:left">32 bit integer</td>
<td style="text-align:left">28</td>
<td style="text-align:left">number of rows  </td>
</tr>
<tr>
<td style="text-align:left">0012</td>
<td style="text-align:left">32 bit integer</td>
<td style="text-align:left">28</td>
<td style="text-align:left">number of columns  </td>
</tr>
<tr>
<td style="text-align:left">0016</td>
<td style="text-align:left">unsigned byte</td>
<td style="text-align:left">??</td>
<td style="text-align:left">pixel  </td>
</tr>
<tr>
<td style="text-align:left">0017</td>
<td style="text-align:left">unsigned byte</td>
<td style="text-align:left">??</td>
<td style="text-align:left">pixel  </td>
</tr>
<tr>
<td style="text-align:left">……..</td>
<td style="text-align:left"></td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">xxxx</td>
<td style="text-align:left">unsigned byte</td>
<td style="text-align:left">??</td>
<td style="text-align:left">pixel</td>
</tr>
</tbody>
</table>
</div>
<p>Pixels are organized row-wise. Pixel values are 0 to 255. 0 means background (white), 255 means foreground (black).</p>
<p>数据开头的4个字节是magicnumber，然后的4个字节是int型的总的图片数量，然后的4个字节是row，再后面的是column数， 这样我们可以通过row x column得出一张图片的所需字节数。这里是 28 x 28 = 784</p>
<p>数值是从0-255的灰度表示0表示白色，255是黑色.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">//一张图片就是一行，这里会把图片的二位数组形式转换为一维数组</span></span><br><span class="line">    <span class="comment">//例如28*28的图片，展开变成1*784的数组。</span></span><br><span class="line">    <span class="comment">//做了scalling</span></span><br><span class="line">    <span class="comment">//数据来源 http://yann.lecun.com/exdb/mnist/</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="type">double</span>[][] readFeatruesFromFile(String fileName) <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">       <span class="type">FileInputStream</span> <span class="variable">fin</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(fileName);</span><br><span class="line">       <span class="keyword">if</span>(fin != <span class="literal">null</span>)&#123;</span><br><span class="line">            <span class="type">BufferedInputStream</span> <span class="variable">bufIns</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferedInputStream</span>(fin);</span><br><span class="line">            <span class="type">byte</span>[] header = <span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">16</span>];</span><br><span class="line">            bufIns.read(header,<span class="number">0</span>,header.length);</span><br><span class="line"></span><br><span class="line">            <span class="type">int</span> <span class="variable">magic</span> <span class="operator">=</span> byte2Int(header,<span class="number">0</span>);</span><br><span class="line">            <span class="type">int</span> <span class="variable">row</span> <span class="operator">=</span> byte2Int(header, <span class="number">8</span>);</span><br><span class="line">            <span class="type">int</span> <span class="variable">col</span> <span class="operator">=</span> byte2Int(header,<span class="number">12</span>);</span><br><span class="line">            <span class="type">int</span> <span class="variable">size</span> <span class="operator">=</span> byte2Int(header,<span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">            <span class="type">int</span> <span class="variable">len</span> <span class="operator">=</span> row*col;</span><br><span class="line">            <span class="type">double</span> data[][] = <span class="keyword">new</span> <span class="title class_">double</span>[size][len];</span><br><span class="line">            <span class="type">byte</span>[] bufBytes = <span class="keyword">new</span> <span class="title class_">byte</span>[len];</span><br><span class="line">            <span class="type">int</span> <span class="variable">idx</span> <span class="operator">=</span> <span class="number">16</span>;</span><br><span class="line">           <span class="comment">// int n = 0;</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span><span class="number">0</span>;i &lt; size; i++)&#123;</span><br><span class="line">                bufIns.read(bufBytes,<span class="number">0</span>,len);</span><br><span class="line">                <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>; (j) &lt; bufBytes.length; j++)&#123;</span><br><span class="line">                    data[i][j] = ((<span class="type">double</span>)byte2Int2(bufBytes,j))/<span class="number">255.0</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> data;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">double</span> dt[][] = &#123;&#125;;</span><br><span class="line">        <span class="keyword">return</span> dt;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>可以发现，这里我们做了feature scalling， 把读到的没个像素的数值除以255. 就可以得到颜色的灰度。把数值控制在0-1之间。</p>
<p>再来看看读取这些数据的最终结果的数据，也就识别后需要比对的数据 </p>
<p>|[offset] |[type]          |[value]          |[description]<br>!:—      |:—             |:—              |:—<br>|0000     |32 bit integer  |0x00000801(2049) |magic number (MSB first)<br>|0004     |32 bit integer  |60000            |number of items<br>|0008     |unsigned byte   |??               |label<br>|0009     |unsigned byte   |??               |label<br>|…….. |                |                 |<br>|xxxx     |unsigned byte   |??               |label</p>
<p>The labels values are 0 to 9.</p>
<p>这里从第三个字节开始就是label，数值从0-9</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="type">double</span>[][] readLabelFromFile(String fileName) <span class="keyword">throws</span> Exception&#123;</span><br><span class="line">       <span class="type">FileInputStream</span> <span class="variable">fin</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">FileInputStream</span>(fileName);</span><br><span class="line">       <span class="keyword">if</span>(fin != <span class="literal">null</span>) &#123;</span><br><span class="line">           <span class="type">BufferedInputStream</span> <span class="variable">bufIns</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">BufferedInputStream</span>(fin);</span><br><span class="line">           <span class="type">byte</span>[] header = <span class="keyword">new</span> <span class="title class_">byte</span>[<span class="number">8</span>];</span><br><span class="line">           bufIns.read(header,<span class="number">0</span>,<span class="number">8</span>);</span><br><span class="line">           <span class="type">int</span> <span class="variable">magic</span> <span class="operator">=</span> byte2Int(header,<span class="number">0</span>);</span><br><span class="line">           <span class="type">int</span> <span class="variable">size</span> <span class="operator">=</span> byte2Int(header,<span class="number">4</span>);</span><br><span class="line">           <span class="type">byte</span>[] y= <span class="keyword">new</span> <span class="title class_">byte</span>[size];</span><br><span class="line">           bufIns.read(y,<span class="number">0</span>,size);</span><br><span class="line">           <span class="type">double</span> data[][] = <span class="keyword">new</span> <span class="title class_">double</span>[size][<span class="number">10</span>];</span><br><span class="line">           <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span><span class="number">0</span>;i &lt; size; i++)&#123;</span><br><span class="line">               <span class="type">int</span> <span class="variable">label</span> <span class="operator">=</span> (<span class="type">int</span>)y[i]&amp;<span class="number">0xff</span>;</span><br><span class="line">               <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">j</span> <span class="operator">=</span> <span class="number">0</span>;  j&lt; <span class="number">10</span> ;j++) &#123;</span><br><span class="line">                   <span class="keyword">if</span>(j==label) &#123;</span><br><span class="line">                       data[i][j] = <span class="number">1</span>;</span><br><span class="line">                   &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                       data[i][j] = <span class="number">0</span>;</span><br><span class="line">                   &#125;</span><br><span class="line">               &#125;</span><br><span class="line">           &#125;</span><br><span class="line">           <span class="keyword">return</span> data;</span><br><span class="line"></span><br><span class="line">       &#125;</span><br><span class="line">       <span class="type">double</span> df[][] = &#123;&#125;;</span><br><span class="line">       <span class="keyword">return</span> df;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>把读取的数据按照行来存放，每一行是一张图片，这就是在java中需要转置的原因。</p>
<h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><p>正则化是在过拟合的时候使用。什么叫过拟合呢？ 就是对样本数据学习得很好，但是对于测试数据，就有很大偏差。</p>
<p>这时候我们可以考虑正则化。通俗的来说，正则化就是减少哪些权重低，但是变化快的因素。也就是“惩罚”这些参数。</p>
<p>我们来看看如何运用到深度学习。请参考理论部分。还记得正则化的最终推论，在改变权重的时候，对权重做了一个处理。</p>
<script type="math/tex; mode=display">
(1-\frac{k\lambda}{n})</script><p>这里k是学习速率，$\lambda$是正则化因子，n是 样本个数。从这里也可以看出，如果样本个数比较多，那么这个因子就接近1，也就是改变不大，也就是说，样本数量足够多，我们其实可以省略正则化。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title function_">updateMiniBatch</span><span class="params">(TrainingData trainingMiniData, <span class="type">double</span> eta, <span class="type">double</span> lambda, <span class="type">double</span> n)</span> &#123;</span><br><span class="line">        <span class="type">Matrix</span> <span class="variable">input</span> <span class="operator">=</span> trainingMiniData.getInput();</span><br><span class="line">        <span class="type">Matrix</span> <span class="variable">output</span> <span class="operator">=</span> trainingMiniData.getOutput();</span><br><span class="line">        <span class="type">Parameter</span> <span class="variable">params</span> <span class="operator">=</span> backprop(input, output);</span><br><span class="line">        <span class="type">double</span> <span class="variable">len</span> <span class="operator">=</span> (<span class="type">double</span>) trainingMiniData.getSize();</span><br><span class="line">        <span class="comment">//after back propagation, update the parameters.</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="built_in">this</span>.sizes.length - <span class="number">1</span>; i++) &#123;</span><br><span class="line">            <span class="built_in">this</span>.bias[i] = <span class="built_in">this</span>.bias[i].subtract(params.getBias()[i].colSum().multiply(eta / len));</span><br><span class="line">            <span class="built_in">this</span>.weights[i] = <span class="built_in">this</span>.weights[i].multiply(<span class="number">1</span> - (eta * lambda) / n).subtract(params.getWeights()[i].multiply</span><br><span class="line">                    (eta / len));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>需要注意的是，偏置不需要正则化。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>如果理论熟悉了，在编码方面，依然会有一些细节要注意，一定要弄清楚各个数值。最后超参数是一个有趣的话题。学习速率，正则化的$\lambda$ 值，模型层次以及每一层的选择，以及批量大小这些都可以自己尝试。</p>
<p>最后还有一些优化可以尝试，比如更好的初始化参数，例如使用 均值为0，标准差为 $\frac{1}{\sqrt(n)}$  的方式初始化权重。这样使得我们的分布更胖一些，学习起来不会很快就饱和了。</p>
<p>如果你的编码正确，正确率在98%以上是没问题的。</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-%E6%AD%A3%E5%88%99%E5%8C%96-%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE/" rel="tag"># 矩阵运算 深度学习 梯度下降 反向传播 正则化  特征缩放</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/12/09/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E7%90%86%E8%AE%BA/" rel="prev" title="深度学习 Step By Step (一，理论篇)">
      <i class="fa fa-chevron-left"></i> 深度学习 Step By Step (一，理论篇)
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/02/22/kademlia%E7%BC%96%E7%A0%81%E8%A7%A3%E8%AF%BB/" rel="next" title="kademlia编码解读">
      kademlia编码解读 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97"><span class="nav-number">1.</span> <span class="nav-text">矩阵运算</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%91%E9%87%8F"><span class="nav-number">1.1.</span> <span class="nav-text">向量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9F%A9%E9%98%B5"><span class="nav-number">1.2.</span> <span class="nav-text">矩阵</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="nav-number">2.</span> <span class="nav-text">初始化</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%AD%A3%E5%90%91%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.</span> <span class="nav-text">正向模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="nav-number">4.</span> <span class="nav-text">随机批量梯度下降算法实现</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="nav-number">5.</span> <span class="nav-text">反向传播算法实现</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB"><span class="nav-number">6.</span> <span class="nav-text">手写数字识别</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">7.</span> <span class="nav-text">正则化</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">8.</span> <span class="nav-text">总结</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Shaoxin Yin"
      src="/images/header.jpg">
  <p class="site-author-name" itemprop="name">Shaoxin Yin</p>
  <div class="site-description" itemprop="description">A programmer's mind</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">24</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">23</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/airuqixue" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;airuqixue" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:shaoxinyin@gmail.com" title="E-Mail → mailto:shaoxinyin@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2014 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Shaoxin Yin</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
